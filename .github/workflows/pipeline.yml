name: Data Pipeline Workflow

on:
  workflow_dispatch: # Trigger manually from GitHub UI
  schedule:
    - cron: "0 3 * * *" # Optional: Run daily at 3 AM UTC

jobs:
  data_pipeline:
    runs-on: ubuntu-latest

    steps:
    # Step 1: Checkout the repository
    - name: Checkout code
      uses: actions/checkout@v3

    # Step 2: Set up Python
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.9'

    # Step 3: Install dependencies
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Set up Google Cloud credentials
      run: echo "${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}" > service-account-key.json

    # Step 4: Fetch data from API and save to Excel
    - name: Fetch data and save to Excel
      run: python scripts/fetch_data.py


    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Google Cloud SDK
      uses: google-github-actions/setup-gcloud@v1
      with:
        project_id: active-tangent-208606  # Replace with your Google Cloud Project ID
        service_account_key: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}
        

    - name: Verify Authentication
      run: |
        gcloud auth list
        echo "Authentication successful!"
        echo "${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}"

    # # Step 6: Upload file to Google Cloud Storage
    # - name: Upload to Google Cloud Storage
    #   env:
    #     GOOGLE_APPLICATION_CREDENTIALS: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}
    #   run: python scripts/upload_to_gcs.py

    - name: Upload file to GCS
      run: |
        echo "Uploading file to GCS..."
        gsutil cp scripts/image.jpg gs://testingslack
